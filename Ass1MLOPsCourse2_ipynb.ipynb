{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass1MLOPsCourse2.ipynb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnvDk5fofWeA5e2sS/kPU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LongNguyen1984/GANLearning/blob/main/Ass1MLOPsCourse2_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-p6T6Gin7a"
      },
      "source": [
        "### Install the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFPfKI_giIJw",
        "outputId": "36aad3f1-897d-48a4-e222-0749be3af090"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip\n",
        "!unzip dataset_diabetes.zip &> 0\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-27 13:58:17--  https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3347213 (3.2M) [application/x-httpd-php]\n",
            "Saving to: ‘dataset_diabetes.zip’\n",
            "\n",
            "dataset_diabetes.zi 100%[===================>]   3.19M  6.14MB/s    in 0.5s    \n",
            "\n",
            "2021-09-27 13:58:18 (6.14 MB/s) - ‘dataset_diabetes.zip’ saved [3347213/3347213]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rS67j0dkU0_"
      },
      "source": [
        "### Get the file for assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUkwGvXbkIEy",
        "outputId": "076b1f31-6258-452a-a8c1-d8c561bae32c"
      },
      "source": [
        "!wget https://github.com/amanchadha/coursera-machine-learning-engineering-for-prod-mlops-specialization/blob/main/C2%20-%20Machine%20Learning%20Data%20Lifecycle%20in%20Production/Week%201/util.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-27 14:03:08--  https://github.com/amanchadha/coursera-machine-learning-engineering-for-prod-mlops-specialization/blob/main/C2%20-%20Machine%20Learning%20Data%20Lifecycle%20in%20Production/Week%201/util.py\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘util.py’\n",
            "\n",
            "util.py                 [ <=>                ] 161.93K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-09-27 14:03:08 (4.22 MB/s) - ‘util.py’ saved [165815]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jjyY12WVjk8a",
        "outputId": "b2cda3aa-cd95-4399-c2a6-1e874ef100f8"
      },
      "source": [
        "!pip install tensorflow-data-validation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-data-validation\n",
            "  Downloading tensorflow_data_validation-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.32\n",
            "  Downloading apache_beam-2.32.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 35.9 MB/s \n",
            "\u001b[?25hCollecting joblib<0.15,>=0.12\n",
            "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting pyarrow<3,>=1\n",
            "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7 MB 79 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.15.0)\n",
            "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (2.6.0)\n",
            "Collecting tfx-bsl<1.4,>=1.3.0\n",
            "  Downloading tfx_bsl-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.0 MB 89 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<1.3,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.2.0)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.1.5)\n",
            "Requirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (0.12.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (3.17.3)\n",
            "Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.19.5)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (3.12.0)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (0.17.4)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (4.1.3)\n",
            "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (3.7.4.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.3.0)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 852 kB/s \n",
            "\u001b[?25hCollecting orjson<4.0\n",
            "  Downloading orjson-3.6.3-cp37-cp37m-manylinux_2_24_x86_64.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 19.8 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 34.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.40.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.7)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (2018.9)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (2.8.2)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.21.0)\n",
            "Collecting google-cloud-vision<2,>=0.38.0\n",
            "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<2,>=0.39.0\n",
            "  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0\n",
            "  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n",
            "\u001b[K     |████████████████████████████████| 169 kB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (4.2.2)\n",
            "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.0.3)\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.8.0)\n",
            "Collecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.35.0)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 70.8 MB/s \n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 72.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 69.8 MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
            "\u001b[K     |████████████████████████████████| 255 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting fasteners>=0.14\n",
            "  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (0.2.8)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (0.4.1)\n",
            "Collecting google-cloud-core<2,>=0.28.1\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.26.3)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (21.0)\n",
            "Collecting proto-plus>=1.15.0\n",
            "  Downloading proto_plus-1.19.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (0.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.32->tensorflow-data-validation) (2.0.6)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.6.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (2.6.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (2.6.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (0.37.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (4.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (3.1.1)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.6.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.4,>=1.3.0->tensorflow-data-validation) (1.12.8)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.4,>=1.3.0->tensorflow-data-validation) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.4,>=1.3.0->tensorflow-data-validation) (3.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2->tensorflow-data-validation) (3.5.0)\n",
            "Building wheels for collected packages: avro-python3, dill, future, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=26de84400ec38131a49abdd3e4668142522b6c99aea7474c8b7fc62cd6424238\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=f0e0ce5f18383d81a42fc0306e5300bf1124baaf98316263c75349e39a6c890d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=2f357958bbec825ffbdfedb77a0700ebd4654692e691a3206751c00dc0833d40\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=2c7b6df057ca79578f66558f2edbea834395b8973d7759be19dc0b6c5c49dcb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=09539f038fc084d36da54e8e8d7e51e2c487f2598bb6101988e58767b9455564\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
            "Successfully built avro-python3 dill future google-apitools grpc-google-iam-v1\n",
            "Installing collected packages: requests, grpcio-gcp, pyarrow, proto-plus, orjson, hdfs, grpc-google-iam-v1, google-cloud-core, future, fasteners, fastavro, dill, avro-python3, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-apitools, apache-beam, tensorflow-serving-api, tfx-bsl, joblib, tensorflow-data-validation\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.32.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.4.5 fasteners-0.16.3 future-0.18.2 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-core-1.7.2 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-recommendations-ai-0.2.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 joblib-0.14.1 orjson-3.6.3 proto-plus-1.19.0 pyarrow-2.0.0 requests-2.26.0 tensorflow-data-validation-1.3.0 tensorflow-serving-api-2.6.0 tfx-bsl-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZiS6STXk5_I"
      },
      "source": [
        "# Import packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tempfile, urllib, zipfile\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow_data_validation.utils import slicing_util\n",
        "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics\n",
        "\n",
        "# Set TF's logger to only display errors to avoid internal warnings being shown\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or7Vh6SEpfqt"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjTgs7yrlEbv",
        "outputId": "a719b3ea-3696-4a37-8e4a-08583b91b634"
      },
      "source": [
        "# Read the dataset and recognize the missing data that in encoded with '?' as Nan\n",
        "\n",
        "df = pd.read_csv('dataset_diabetes/diabetic_data.csv', header=0, na_values ='?')\n",
        "\n",
        "# preview the dataset\n",
        "df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 101766 entries, 0 to 101765\n",
            "Data columns (total 50 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   encounter_id              101766 non-null  int64 \n",
            " 1   patient_nbr               101766 non-null  int64 \n",
            " 2   race                      99493 non-null   object\n",
            " 3   gender                    101766 non-null  object\n",
            " 4   age                       101766 non-null  object\n",
            " 5   weight                    3197 non-null    object\n",
            " 6   admission_type_id         101766 non-null  int64 \n",
            " 7   discharge_disposition_id  101766 non-null  int64 \n",
            " 8   admission_source_id       101766 non-null  int64 \n",
            " 9   time_in_hospital          101766 non-null  int64 \n",
            " 10  payer_code                61510 non-null   object\n",
            " 11  medical_specialty         51817 non-null   object\n",
            " 12  num_lab_procedures        101766 non-null  int64 \n",
            " 13  num_procedures            101766 non-null  int64 \n",
            " 14  num_medications           101766 non-null  int64 \n",
            " 15  number_outpatient         101766 non-null  int64 \n",
            " 16  number_emergency          101766 non-null  int64 \n",
            " 17  number_inpatient          101766 non-null  int64 \n",
            " 18  diag_1                    101745 non-null  object\n",
            " 19  diag_2                    101408 non-null  object\n",
            " 20  diag_3                    100343 non-null  object\n",
            " 21  number_diagnoses          101766 non-null  int64 \n",
            " 22  max_glu_serum             101766 non-null  object\n",
            " 23  A1Cresult                 101766 non-null  object\n",
            " 24  metformin                 101766 non-null  object\n",
            " 25  repaglinide               101766 non-null  object\n",
            " 26  nateglinide               101766 non-null  object\n",
            " 27  chlorpropamide            101766 non-null  object\n",
            " 28  glimepiride               101766 non-null  object\n",
            " 29  acetohexamide             101766 non-null  object\n",
            " 30  glipizide                 101766 non-null  object\n",
            " 31  glyburide                 101766 non-null  object\n",
            " 32  tolbutamide               101766 non-null  object\n",
            " 33  pioglitazone              101766 non-null  object\n",
            " 34  rosiglitazone             101766 non-null  object\n",
            " 35  acarbose                  101766 non-null  object\n",
            " 36  miglitol                  101766 non-null  object\n",
            " 37  troglitazone              101766 non-null  object\n",
            " 38  tolazamide                101766 non-null  object\n",
            " 39  examide                   101766 non-null  object\n",
            " 40  citoglipton               101766 non-null  object\n",
            " 41  insulin                   101766 non-null  object\n",
            " 42  glyburide-metformin       101766 non-null  object\n",
            " 43  glipizide-metformin       101766 non-null  object\n",
            " 44  glimepiride-pioglitazone  101766 non-null  object\n",
            " 45  metformin-rosiglitazone   101766 non-null  object\n",
            " 46  metformin-pioglitazone    101766 non-null  object\n",
            " 47  change                    101766 non-null  object\n",
            " 48  diabetesMed               101766 non-null  object\n",
            " 49  readmitted                101766 non-null  object\n",
            "dtypes: int64(13), object(37)\n",
            "memory usage: 38.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyRDsACTqnx8"
      },
      "source": [
        "### Data split\n",
        "will split the dataset into:\n",
        "\n",
        "* 70% training set\n",
        "* 15% evaluation set\n",
        "* 15% serving set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdzEWe1zqp_E"
      },
      "source": [
        "def prepare_data_splits_from_dataframe(df):\n",
        "  '''\n",
        "    Splits a Pandas Dataframe into training, evaluation and serving sets.\n",
        "\n",
        "    Parameters:\n",
        "            df : pandas dataframe to split\n",
        "\n",
        "    Returns:\n",
        "            train_df: Training dataframe(70% of the entire dataset)\n",
        "            eval_df: Evaluation dataframe (15% of the entire dataset) \n",
        "            serving_df: Serving dataframe (15% of the entire dataset, label column dropped)\n",
        "    '''\n",
        "    # 70% of records for generating the training set\n",
        "  train_len = int(len(df)*0.7)\n",
        "\n",
        "    # Remaining 30% of records for generating the evaluation and serving sets\n",
        "  eval_serv_len = len(df) - train_len\n",
        "\n",
        "  eval_len = eval_serv_len//2\n",
        "\n",
        "  ser_len = eval_serv_len - eval_len\n",
        "\n",
        "    # Sample the train, validation and serving set.\n",
        "  train_df = df.iloc[:train_len].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  eval_df = df.iloc[train_len:train_len + eval_len].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  ser_df = df.iloc[train_len + eval_len: train_len + eval_len + ser_len].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Serving data emuates the data that would be submitted for predictions so it not have the label column.\n",
        "  ser_df = ser_df.drop(['readmitted'], axis=1)\n",
        "\n",
        "  return train_df, eval_df, ser_df\n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itnh8-jOwPXH",
        "outputId": "fae016f9-1750-48df-bfb7-2639d6c39323"
      },
      "source": [
        "## Split the datasset\n",
        "\n",
        "train_df, eval_df, ser_df = prepare_data_splits_from_dataframe(df)\n",
        "print('Training dataset has {} records\\n Validation dataset has {} records\\nServing dataset has {} records'.format(len(train_df), len(eval_df), len(ser_df)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset has 71236 records\n",
            " Validation dataset has 15265 records\n",
            "Serving dataset has 15265 records\n"
          ]
        }
      ]
    }
  ]
}